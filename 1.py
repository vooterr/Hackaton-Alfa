import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error
import xgboost as xgb


def wmae(y_true, y_pred, weight):
    """–í–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"""
    return np.sum(np.abs(y_true - y_pred) * weight) / np.sum(weight)


def quick_preprocess_raw(df):
    """–ë—ã—Å—Ç—Ä–∞—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
    df_clean = df.copy()
    for col in df_clean.columns:
        if df_clean[col].dtype == 'object':
            try:
                temp = df_clean[col].astype(str).str.replace(',', '.', regex=False)
                df_clean[col] = pd.to_numeric(temp, errors='ignore')
            except Exception:
                df_clean[col] = df_clean[col].fillna('Unknown')

        if df_clean[col].isna().any():
            if df_clean[col].dtype == 'object':
                df_clean[col] = df_clean[col].fillna('Unknown')
            else:
                df_clean[col] = df_clean[col].fillna(df_clean[col].median())
    return df_clean


def evaluate_xgb_gpu(train_data, test_size=0.2, weight_col='w'):
    """–û—Ü–µ–Ω–∫–∞ XGBoost –Ω–∞ GPU —Å —É—á–µ—Ç–æ–º –≤–µ—Å–æ–≤"""
    X = train_data.drop(columns=['target'])
    y = train_data['target']

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    if weight_col in X.columns:
        weights = X[weight_col].values
        X = X.drop(columns=[weight_col])
    else:
        weights = np.ones(len(X))

    # –¢–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X = X[numeric_cols]

    X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(
        X, y, weights,
        test_size=test_size,
        random_state=42
    )

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π —Ä–∞–±–æ—Ç—ã XGBoost
    w_train_normalized = w_train / w_train.mean()
    w_val_normalized = w_val / w_val.mean()

    # –í–ê–ñ–ù–û: –ø–µ—Ä–µ–¥–∞—ë–º weight –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ DMatrix
    dtrain = xgb.DMatrix(X_train.values, label=y_train.values, weight=w_train_normalized)
    dval = xgb.DMatrix(X_val.values, label=y_val.values, weight=w_val_normalized)

    params = {
        'tree_method': 'hist',  # –∏–ª–∏ 'gpu_hist' –µ—Å–ª–∏ –µ—Å—Ç—å GPU
        'max_depth': 6,
        'learning_rate': 0.1,
        'objective': 'reg:squarederror',
        'eval_metric': 'mae'
    }

    evals = [(dtrain, 'train'), (dval, 'val')]
    model = xgb.train(
        params=params,
        dtrain=dtrain,
        num_boost_round=100,
        evals=evals,
        verbose_eval=False
    )

    y_pred = model.predict(dval)

    mae = mean_absolute_error(y_val, y_pred)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WMAE
    wmae_score = wmae(y_val.values, y_pred, w_val)

    return mae, wmae_score, X_train.shape[1]


def evaluate_rf_on_dataset(train_data, test_size=0.2, weight_col='w'):
    """–û—Ü–µ–Ω–∫–∞ Random Forest –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ (CPU –≤–µ—Ä—Å–∏—è)"""
    X = train_data.drop(columns=['target'])
    y = train_data['target']

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–µ—Å–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
    if weight_col in X.columns:
        weights = X[weight_col].values
        X = X.drop(columns=[weight_col])
    else:
        weights = np.ones(len(X))

    numeric_cols = X.select_dtypes(include=[np.number]).columns
    X = X[numeric_cols]

    X_train, X_val, y_train, y_val, w_train, w_val = train_test_split(
        X, y, weights,
        test_size=test_size,
        random_state=42
    )

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞ –¥–ª—è RF
    w_train_normalized = w_train / w_train.mean()

    rf = RandomForestRegressor(
        n_estimators=100,
        random_state=42,
        n_jobs=-1
    )
    # –í–µ—Å–∞ –¥–ª—è RF
    rf.fit(X_train, y_train, sample_weight=w_train_normalized)

    y_pred = rf.predict(X_val)

    mae = mean_absolute_error(y_val, y_pred)
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –≤–µ—Å–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è WMAE
    wmae_score = wmae(y_val.values, y_pred, w_val)

    return mae, wmae_score, X_train.shape[1]


# ===================== –û–°–ù–û–í–ù–û–ï –°–†–ê–í–ù–ï–ù–ò–ï =====================

print("üîç –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô –ù–ê –†–ê–ó–ù–´–• –î–ê–¢–ê–°–ï–¢–ê–• (–° –£–ß–ï–¢–û–ú –í–ï–°–û–í)")
print("=" * 60)

print("1. –û–ß–ò–©–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï (XGBoost):")
train_raw = pd.read_csv('data/hackathon_income_train_cleared.csv')
train_raw_clean = quick_preprocess_raw(train_raw)
mae_raw, wmae_raw, features_raw = evaluate_xgb_gpu(train_raw_clean)
print(f"   MAE: {mae_raw:.4f}, WMAE: {wmae_raw:.4f}, –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_raw}")

print("\n3. –ò–ú–ü–£–¢–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï (XGBoost):")
train_imputed = pd.read_csv('data/hackathon_income_train_imputed.csv')
mae_imputed, wmae_imputed, features_imputed = evaluate_xgb_gpu(train_imputed)
print(f"   MAE: {mae_imputed:.4f}, WMAE: {wmae_imputed:.4f}, –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_imputed}")

print("\n4. –ò–ú–ü–£–¢–ò–†–û–í–ê–ù–ù–´–ï –î–ê–ù–ù–´–ï (Random Forest):")
mae_rf, wmae_rf, features_rf = evaluate_rf_on_dataset(train_imputed)
print(f"   MAE: {mae_rf:.4f}, WMAE: {wmae_rf:.4f}, –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {features_rf}")

print("\nüèÜ –ò–¢–û–ì–ò:")
results = {
    'XGBoost (–æ—á–∏—â)': wmae_raw,
    'XGBoost (–∏–º–ø—É—Ç)': wmae_imputed,
    'RF (–∏–º–ø—É—Ç)': wmae_rf
}

best_model = min(results, key=results.get)
print(f"–õ–£–ß–®–ê–Ø –ú–û–î–ï–õ–¨: {best_model} (WMAE: {results[best_model]:.4f})")
